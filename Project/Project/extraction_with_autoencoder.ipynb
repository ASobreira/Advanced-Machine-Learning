{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import cv2\n",
    "from sklearn.svm import SVC\n",
    "from skimage.feature import hog\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from skimage.color import rgb2gray\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Flatten, Dense, Dropout, GlobalAveragePooling2D\n",
    "from keras.applications import VGG16\n",
    "from skimage.feature import hog\n",
    "from skimage import data, exposure\n",
    "from keras.models import Model\n",
    "from keras import layers, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 23708\n",
      "100_0_0_20170112213500903.jpg.chip.jpg\n"
     ]
    }
   ],
   "source": [
    "#path martim\n",
    "path = \"../../../../DadosProj/UTKFace/\"\n",
    "#path alex\n",
    "#ath = \"../../../data_project/UTKFace/\"\n",
    "\n",
    "files = os.listdir(path)\n",
    "size = len(files)\n",
    "print(\"Total samples:\",size)\n",
    "print(files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "ages = []\n",
    "genders = []\n",
    "ethnicities = []\n",
    "counter = 0\n",
    "\n",
    "for file in files:\n",
    "    if counter == 500:\n",
    "        break\n",
    "    \n",
    "    image = cv2.imread(path+file)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, dsize=(200, 200))\n",
    "    #image = image.astype('float32') / 255.0 #normalizing values on each point \n",
    "    image = image / 255.0\n",
    "\n",
    "    #verificar se imagens todas estao no range 0 a 255 (se sao rgb)\n",
    "\n",
    "    images.append(image)\n",
    "    delimit = file.split('_')\n",
    "    ages.append(int(delimit[0]))\n",
    "    # first number is age (0-116 possible values)\n",
    "    genders.append(int(delimit[1]))\n",
    "    # second number is gender (0 for male, 1 for female)\n",
    "    ethnicities.append(int(delimit[2]))\n",
    "    # third number is ethnicity (0 for white, 1 for black, 2 for asian, 3 for indian, 4 for any other ethnicity)\n",
    "\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_img(img, idx, isgray = False):\n",
    "    print(\"Gender:\", genders[idx], \"Age:\", ages[idx], \"Ethnicity:\", ethnicities[idx])\n",
    "    if isgray:\n",
    "        plt.imshow(img[idx],  cmap=\"gray\")\n",
    "    else:\n",
    "        plt.imshow(img[idx],  cmap=\"gray\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display_img(images, 77)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract HOG features from the images\n",
    "hog_features = []\n",
    "for img in images:\n",
    "    #hog_feature = hog(img, orientations=9, pixels_per_cell=(8, 8),\n",
    "    #                cells_per_block=(2, 2), transform_sqrt=True, feature_vector=True, multichannel=True, channel_axis=2)\n",
    "    hog_feature = hog(img, orientations=9, pixels_per_cell=(8, 8),\n",
    "                    cells_per_block=(2, 2), transform_sqrt=True, feature_vector=True, channel_axis=2)\n",
    "    \n",
    "    hog_features.append(hog_feature)\n",
    "hog_features = np.array(hog_features)\n",
    "\n",
    "# Normalize the features to have zero mean and unit variance\n",
    "mean = np.mean(hog_features, axis=0)\n",
    "std = np.std(hog_features, axis=0)\n",
    "hog_features_norm = (hog_features - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(hog_features_norm, genders, test_size=0.2, random_state=42)\n",
    "#print(\"Samples in Training:\",X_train.shape[0])\n",
    "#print(\"Samples in Testing:\",X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.81\n"
     ]
    }
   ],
   "source": [
    "svc = SVC(kernel='linear')\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the classifier on the testing data\n",
    "accuracy = svc.score(X_test, y_test)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for this the order is first the split of the data then using autoencoder on train_x\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, genders, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], X_train.shape[2], 3))\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], X_test.shape[2], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Autoencoder(Model):\n",
    "#   def __init__(self):\n",
    "#     super(Autoencoder, self).__init__()\n",
    "\n",
    "#     self.encoder = tf.keras.Sequential([\n",
    "#       #didnt increase filter count in encoder and decrease equally in decoder because\n",
    "#       #it can lead to overfitting\n",
    "\n",
    "#       layers.Input(shape=(200, 200, 3)),\n",
    "#       layers.Conv2D(64, (3,3), activation='relu', padding='same'),\n",
    "#       layers.MaxPooling2D((2,2), padding='same'),\n",
    "#       layers.Conv2D(32, (3,3), activation='relu', padding='same'),\n",
    "#       layers.MaxPooling2D((2,2), padding='same'),\n",
    "#       layers.Conv2D(16, (3,3), activation='relu', padding='same'),\n",
    "#       layers.MaxPooling2D((2,2), padding='same'),])\n",
    "    \n",
    "#     self.decoder = tf.keras.Sequential([\n",
    "#       #having conv2D here helps to avoid checkerboard artifacts\n",
    "#       #which are alternating points of colors (like checkerboard)\n",
    "#       #that are consequence of deconvolutioning an image and cause \n",
    "#       #loss of quality in final reconstruction\n",
    "\n",
    "#       layers.Conv2D(16, (3,3), activation='relu', padding='same'),\n",
    "#       layers.Conv2DTranspose(32, (3,3), activation='relu', padding='same'),\n",
    "#       layers.Conv2DTranspose(64, (3,3), activation='relu', padding='same'),\n",
    "#       layers.Conv2DTranspose(3, (3,3), activation='sigmoid', padding='same'),])\n",
    "    \n",
    "#       #sigmoid is required for final layer since we are evaluating with\n",
    "#       #binary cross entropy\n",
    "    \n",
    "#       #maybe use strides in the layers too\n",
    "\n",
    "#   def call(self, x):\n",
    "#     encoded = self.encoder(x)\n",
    "#     decoded = self.decoder(encoded)\n",
    "#     return decoded\n",
    "\n",
    "# # binary cross entropy is good for image feature extraction especially when images are normalized\n",
    "\n",
    "# autoencoder = Autoencoder()\n",
    "# autoencoder.compile(optimizer='adam', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_41 (InputLayer)       [(None, 200, 200, 3)]     0         \n",
      "                                                                 \n",
      " conv2d_116 (Conv2D)         (None, 100, 100, 64)      1792      \n",
      "                                                                 \n",
      " conv2d_117 (Conv2D)         (None, 50, 50, 32)        18464     \n",
      "                                                                 \n",
      " conv2d_118 (Conv2D)         (None, 25, 25, 16)        4624      \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 10000)             0         \n",
      "                                                                 \n",
      " dense_68 (Dense)            (None, 2000)              20002000  \n",
      "                                                                 \n",
      " dense_69 (Dense)            (None, 10000)             20010000  \n",
      "                                                                 \n",
      " reshape_23 (Reshape)        (None, 25, 25, 16)        0         \n",
      "                                                                 \n",
      " conv2d_transpose_18 (Conv2D  (None, 50, 50, 32)       4640      \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " conv2d_transpose_19 (Conv2D  (None, 100, 100, 64)     18496     \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " conv2d_transpose_20 (Conv2D  (None, 200, 200, 3)      1731      \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 40,061,747\n",
      "Trainable params: 40,061,747\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input = layers.Input(shape=(200, 200, 3))\n",
    "\n",
    "#valor alto apanha artefactos, e valor baixo nao captura bem as features da imagem\n",
    "latent_dim = 2000\n",
    "\n",
    "# Encoder\n",
    "\n",
    "#didnt increase filter count in encoder and decrease equally in decoder because\n",
    "#it can lead to overfitting\n",
    "\n",
    "x = layers.Conv2D(64, (3,3), activation='relu', padding='same', strides = 2)(input)\n",
    "#x = layers.MaxPooling2D((2,2), padding='same')(x)#dividir imagem por factor de 2\n",
    "x = layers.Conv2D(32, (3,3), activation='relu', padding='same', strides = 2)(x)#reduziu se nr de filtros para 32 porque foi decidido factor de 32 filtros\n",
    "#x = layers.MaxPooling2D((2,2), padding='same')(x)\n",
    "x = layers.Conv2D(16, (3,3), activation='relu', padding='same', strides = 2)(x)\n",
    "#x = layers.MaxPooling2D((2,2), padding='same')(x)#25*25*16=10000\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(latent_dim, activation='relu')(x)\n",
    "\n",
    "# Decoder\n",
    "\n",
    "#having conv2D here helps to avoid checkerboard artifacts\n",
    "#which are alternating points of colors (like checkerboard)\n",
    "#that are consequence of deconvolutioning an image and cause \n",
    "#loss of quality in final reconstruction\n",
    "#x = layers.Conv2D(16, (3,3), activation='relu', padding='same')(x)\n",
    "\n",
    "#equivalente a fazer dense layer no fim do encoder e no inicio do decoder\n",
    "#fazer a layer dense fica um vector\n",
    "#fazer reshape depois do 2 dense para voltar a ter imagem do vetor criado\n",
    "\n",
    "x = layers.Dense(25*25*16, activation='relu')(x)\n",
    "x = layers.Reshape((25,25,16))(x)\n",
    "#x = layers.UpSampling2D((2,2))(x)\n",
    "x = layers.Conv2DTranspose(32, (3, 3), activation=\"relu\", padding=\"same\", strides = 2)(x)\n",
    "#x = layers.UpSampling2D((2,2))(x)\n",
    "x = layers.Conv2DTranspose(64, (3,3), activation='relu', padding='same', strides = 2)(x)\n",
    "#x = layers.UpSampling2D((2,2))(x)\n",
    "output = layers.Conv2DTranspose(3, (3,3), activation='sigmoid', padding='same', strides = 2)(x)\n",
    "\n",
    "\n",
    "#maybe use strides in the layers too\n",
    "#example:\n",
    "#x = layers.Conv2D(16, (3,3), activation='relu', padding='same', strides = 2)(x)\n",
    "\n",
    "# Autoencoder\n",
    "autoencoder = Model(input, output)\n",
    "\n",
    "# binary cross entropy is good for image feature extraction especially when images are normalized\n",
    "autoencoder.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "13/13 [==============================] - 7s 520ms/step - loss: 0.6892 - val_loss: 0.6767\n",
      "Epoch 2/2\n",
      "13/13 [==============================] - 6s 500ms/step - loss: 0.6684 - val_loss: 0.6666\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1510dcd5600>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using validation_data is good for detecting overfitting as \n",
    "#it does early stopping when the performance is no longer improving\n",
    "\n",
    "autoencoder.fit(x = X_train, \n",
    "                y = X_train,\n",
    "                epochs = 2,\n",
    "                shuffle = True,\n",
    "                batch_size = 32,\n",
    "                validation_data = (X_test, X_test),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 1s 91ms/step\n",
      "4/4 [==============================] - 0s 72ms/step\n"
     ]
    }
   ],
   "source": [
    "#encoder = Model(inputs=input, outputs=autoencoder.get_layer(\"conv2d_transpose_55\").output)\n",
    "encoded_train = autoencoder.predict(X_train)\n",
    "encoded_test = autoencoder.predict(X_test)\n",
    "\n",
    "encoded_train = encoded_train.reshape((encoded_train.shape[0], -1))\n",
    "encoded_test = encoded_test.reshape((encoded_test.shape[0], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.62\n"
     ]
    }
   ],
   "source": [
    "clf = SVC(kernel='linear')\n",
    "clf.fit(encoded_train, y_train)\n",
    "\n",
    "# Evaluate the classifier on the testing data\n",
    "accuracy = clf.score(encoded_test, y_test)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Orb (por de parte por agora...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display_img(images, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#orb works with greyscale images\n",
    "grey_images = np.array([rgb2gray(image) for image in images]) #just in case we need the images in greys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display_img(grey_images, 6, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#images = map(float, images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<map at 0x18fd475e050>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'map' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m orb \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mORB_create()\n\u001b[1;32m----> 3\u001b[0m image \u001b[39m=\u001b[39m images[\u001b[39m1\u001b[39;49m]\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39muint8, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m      5\u001b[0m kp \u001b[39m=\u001b[39m orb\u001b[39m.\u001b[39mdetect(image, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m      6\u001b[0m kp, des \u001b[39m=\u001b[39m orb\u001b[39m.\u001b[39mdetectAndCompute(image, kp)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'map' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "orb = cv2.ORB_create()\n",
    "\n",
    "image = images[1].astype(np.uint8, copy=False)\n",
    "\n",
    "#????? problema aqui\n",
    "kp = orb.detect(image, None)\n",
    "kp, des = orb.detectAndCompute(image, kp)\n",
    "\n",
    "img2 = cv2.drawKeypoints(image, kp, None, color=(0,255,0), flags=0)\n",
    "\n",
    "plt.imshow(img2, image.astype(np.uint8))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "#retirar keypoints aleatoriamente de imagens com keypoints acima de um limiar decidido\n",
    "#e ter uma quantidade minima definida para os keypoints\n",
    "\n",
    "orb = cv2.ORB_create()\n",
    "orb_features = []\n",
    "Genders = []\n",
    "MIN_KEYPOINTS = 500\n",
    "MAX_KEYPOINTS = 500\n",
    "\n",
    "for i, img in enumerate(images):\n",
    "    \n",
    "    keypoints, descriptors = orb.detectAndCompute(img, None)\n",
    "    keypoints, descriptors = orb.compute(images[1], keypoints) #se houver problemas nesta linha ver esta duvida https://stackoverflow.com/questions/55128386/python-opencv-depth-of-image-unsupported-cv-64f\n",
    "    \n",
    "    if len(keypoints) < MIN_KEYPOINTS:\n",
    "        continue  \n",
    "    if len(keypoints) > MAX_KEYPOINTS:\n",
    "        continue \n",
    "    \n",
    "    orb_features.append(np.ndarray.flatten(descriptors))\n",
    "    Genders.append(genders[i]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, arr in enumerate(orb_features):\n",
    "    print(f\"Descriptor array {i} shape: {arr.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean(orb_features, axis=0)\n",
    "std = np.std(orb_features, axis=0)\n",
    "orb_features_norm = (orb_features - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(orb_features_norm, Genders, test_size=0.2, random_state=42)\n",
    "print(\"Samples in Training:\",X_train.shape[0])\n",
    "print(\"Samples in Testing:\",X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC()\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "accuracy = svc.score(X_test, y_test)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cnn classification of genders using data from hog feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(hog_features_norm, genders, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20736,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape (Reshape)           (None, 144, 144, 1)       0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 72, 72, 1)        0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 72, 72, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 36, 36, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 36, 36, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 18, 18, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 20736)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                1327168   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,346,049\n",
      "Trainable params: 1,346,049\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn = Sequential()\n",
    "\n",
    "#tratado\n",
    "cnn.add(layers.Reshape((144, 144, 1), input_shape=(20736,)))\n",
    "#cnn.add(layers.Conv2D(16, (3, 3), padding='same', activation='relu', input_shape=X_train[1].shape))\n",
    "\n",
    "cnn.add(layers.MaxPooling2D((2, 2)))\n",
    "cnn.add(layers.Conv2D(32, (3, 3), padding='same', activation='relu'))\n",
    "cnn.add(layers.MaxPooling2D(2,2))\n",
    "cnn.add(layers.Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "cnn.add(layers.MaxPooling2D(2,2))\n",
    "cnn.add(Flatten())\n",
    "cnn.add(Dense(64, activation='relu'))\n",
    "cnn.add(layers.Dropout(0.2))\n",
    "\n",
    "#output layer with 2 filters for number of classes the model will choose to do predictions\n",
    "#and since gender is just binary classification sigmoid is appropriate\n",
    "#when doing classification for age (wont be binary classification) then softmax is good choice\n",
    "cnn.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.asarray(X_train)\n",
    "y_train = np.asarray(y_train)\n",
    "\n",
    "X_test = np.asarray(X_test)\n",
    "y_test = np.asarray(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "13/13 [==============================] - 1s 56ms/step - loss: 0.8877 - val_loss: 0.6984\n",
      "Epoch 2/5\n",
      "13/13 [==============================] - 1s 53ms/step - loss: 0.6835 - val_loss: 0.6868\n",
      "Epoch 3/5\n",
      "13/13 [==============================] - 1s 51ms/step - loss: 0.6423 - val_loss: 0.6417\n",
      "Epoch 4/5\n",
      "13/13 [==============================] - 1s 51ms/step - loss: 0.5858 - val_loss: 0.6751\n",
      "Epoch 5/5\n",
      "13/13 [==============================] - 1s 51ms/step - loss: 0.4846 - val_loss: 0.8054\n"
     ]
    }
   ],
   "source": [
    "cnn.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\n",
    "\n",
    "history = cnn.fit(X_train, y_train,\n",
    "                    epochs=5,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = cnn.evaluate(X_test, y_test)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "duvidas tiradas com prof"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "esta autoencoder,vgg16,hog a funfar \n",
    "ver svm e cnn results\n",
    "correr agora só com gender\n",
    "\n",
    "(prof diz que é preferivel usar o que temos acima e se houver tempo e tudo acima\n",
    "funcionar a dar resultados e tudo entao só depois tentar fazer o orb)\n",
    "\n",
    "---\n",
    "\n",
    "quando isto tudo acima estiver ok tentar tudo denovo para dar predict de idades\n",
    "que pode ser feito de 2 maneiras:\n",
    "regressao \n",
    "ou\n",
    "idades agrupadas em intervalos (10 a 10 anos por exemplo)\n",
    "\n",
    "----\n",
    "\n",
    "tentar no fit dos modelos(cnn e encoder):\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping\n",
    "\n",
    "https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/\n",
    "\n",
    "quando tiver tudo a dar para fazer early stopping com o parametro de patience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
